{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from google.oauth2 import service_account\n",
    "import gcsfs\n",
    "import dask.dataframe as dd\n",
    "\n",
    "BUCKET_NAME = 'events_trabalho_cdle'\n",
    "TRAINING_FILE = 'training_padded.csv'\n",
    "TESTING_FILE = 'testing_padded.csv'\n",
    "\n",
    "# Authenticate using the service account key file with specified scopes\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/Users/rcs/Downloads/cdla-trabalho-db9f3d742a66.json',\n",
    "    scopes=[\"https://www.googleapis.com/auth/devstorage.read_write\"]\n",
    ")\n",
    "fs = gcsfs.GCSFileSystem(project='cdla-trabalho', token=credentials)\n",
    "# Load CSV files into Dask DataFrames using the authenticated GCSFileSystem\n",
    "storage_options = {'project': 'cdla-trabalho', 'token': credentials}\n",
    "ddf_train = dd.read_csv(f'gs://{BUCKET_NAME}/{TRAINING_FILE}', storage_options=storage_options)\n",
    "ddf_test = dd.read_csv(f'gs://{BUCKET_NAME}/{TESTING_FILE}', storage_options=storage_options)\n",
    "\n",
    "# Convert Dask DataFrames to Pandas DataFrames\n",
    "train_df = ddf_train.compute()\n",
    "test_df = ddf_test.compute()\n",
    "\n",
    "# Optionally, you can now convert Pandas DataFrames to Modin DataFrames if needed\n",
    "train_df_modin = pd.DataFrame(train_df)\n",
    "test_df_modin = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6998, 1, 11)\n",
      "(1750, 1, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: <function Series.tolist> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: <function Series.tolist> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
     ]
    }
   ],
   "source": [
    "# Function to parse sequence string to list of sublists\n",
    "def parse_sequence(sequence):\n",
    "    try:\n",
    "        # Convert the string representation of the list to an actual list\n",
    "        sequence_list = ast.literal_eval(sequence)\n",
    "        # Flatten each sublist: [timestamp, (a, b), (c, d)] -> [timestamp, a, b, c, d]\n",
    "        flattened_sequence = []\n",
    "        for sublist in sequence_list:\n",
    "            if sublist == [-1]:\n",
    "                flattened_sequence.append([-1] * 11)  # Adjust the length as per the maximum possible sublist length\n",
    "            else:\n",
    "                flattened_sublist = [sublist[0]] + [item for tup in sublist[1:] for item in tup]\n",
    "                if len(flattened_sublist) < 11:\n",
    "                    flattened_sublist += [-1] * (11 - len(flattened_sublist))\n",
    "                flattened_sequence.append(flattened_sublist)\n",
    "        return flattened_sequence\n",
    "    except (SyntaxError, ValueError):\n",
    "        return [[-1] * 11]  # Adjust the length as per the maximum possible sublist length\n",
    "\n",
    "# Apply the parse_sequence function\n",
    "train_df_modin['Padded_Sequence'] = train_df_modin['Padded_Sequence'].apply(parse_sequence)\n",
    "test_df_modin['Padded_Sequence'] = test_df_modin['Padded_Sequence'].apply(parse_sequence)\n",
    "\n",
    "# Ensure all sequences have the same length\n",
    "max_length = max(train_df_modin['Padded_Sequence'].apply(len).max(), test_df_modin['Padded_Sequence'].apply(len).max())\n",
    "input_dim = 11  # This should be the length of the flattened sublist\n",
    "\n",
    "def pad_sequences(sequences, max_length, input_dim):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded_seq = seq + [[-1] * input_dim] * (max_length - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences, dtype=np.float32)\n",
    "\n",
    "X_train = pad_sequences(train_df_modin['Padded_Sequence'].tolist(), max_length, input_dim)\n",
    "y_train = train_df_modin['LOS'].values\n",
    "\n",
    "X_test = pad_sequences(test_df_modin['Padded_Sequence'].tolist(), max_length, input_dim)\n",
    "y_test = test_df_modin['LOS'].values\n",
    "\n",
    "# Print shapes to verify\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sequence length: 1\n",
      "Epoch 1/100\n",
      "438/438 - 5s - 12ms/step - loss: 75.8038 - mae: 5.7209 - val_loss: 48.7984 - val_mae: 4.6746 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.4863 - mae: 4.6215 - val_loss: 48.7600 - val_mae: 4.6182 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.1771 - mae: 4.6470 - val_loss: 48.9073 - val_mae: 4.7513 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.4353 - mae: 4.6549 - val_loss: 48.8165 - val_mae: 4.6911 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.1843 - mae: 4.6572 - val_loss: 48.7553 - val_mae: 4.5964 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.2457 - mae: 4.6636 - val_loss: 49.0021 - val_mae: 4.4113 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.3094 - mae: 4.6458 - val_loss: 48.8088 - val_mae: 4.5044 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8818 - mae: 4.6341 - val_loss: 48.7733 - val_mae: 4.5398 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.0817 - mae: 4.6267 - val_loss: 48.8171 - val_mae: 4.6915 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8150 - mae: 4.6434 - val_loss: 48.7574 - val_mae: 4.5728 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8240 - mae: 4.6356 - val_loss: 48.7879 - val_mae: 4.6634 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9933 - mae: 4.6455 - val_loss: 48.7554 - val_mae: 4.5849 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9727 - mae: 4.6520 - val_loss: 48.7855 - val_mae: 4.5252 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8189 - mae: 4.6235 - val_loss: 48.7568 - val_mae: 4.6067 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9069 - mae: 4.6196 - val_loss: 48.9167 - val_mae: 4.7563 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9127 - mae: 4.6924 - val_loss: 48.7588 - val_mae: 4.6146 - learning_rate: 2.0000e-05\n",
      "Epoch 17/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8713 - mae: 4.6274 - val_loss: 48.7573 - val_mae: 4.6089 - learning_rate: 2.0000e-05\n",
      "Epoch 18/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.1509 - mae: 4.6505 - val_loss: 48.7582 - val_mae: 4.6124 - learning_rate: 2.0000e-05\n",
      "Epoch 19/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.7895 - mae: 4.6335 - val_loss: 48.7556 - val_mae: 4.5988 - learning_rate: 2.0000e-05\n",
      "Epoch 20/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8249 - mae: 4.6315 - val_loss: 48.7592 - val_mae: 4.6159 - learning_rate: 2.0000e-05\n",
      "Epoch 21/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9626 - mae: 4.6493 - val_loss: 48.7559 - val_mae: 4.5803 - learning_rate: 2.0000e-05\n",
      "Epoch 22/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.6828 - mae: 4.6267 - val_loss: 48.7564 - val_mae: 4.5776 - learning_rate: 2.0000e-05\n",
      "Epoch 23/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8739 - mae: 4.6550 - val_loss: 48.7565 - val_mae: 4.5767 - learning_rate: 2.0000e-05\n",
      "Epoch 24/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.7630 - mae: 4.6308 - val_loss: 48.7553 - val_mae: 4.5961 - learning_rate: 2.0000e-05\n",
      "Epoch 25/100\n",
      "438/438 - 1s - 3ms/step - loss: 52.8320 - mae: 4.6544 - val_loss: 48.7682 - val_mae: 4.5474 - learning_rate: 2.0000e-05\n",
      "Epoch 26/100\n",
      "438/438 - 1s - 3ms/step - loss: 52.7731 - mae: 4.6052 - val_loss: 48.7552 - val_mae: 4.5903 - learning_rate: 1.0000e-05\n",
      "Epoch 27/100\n",
      "438/438 - 1s - 3ms/step - loss: 52.6460 - mae: 4.6254 - val_loss: 48.7566 - val_mae: 4.6059 - learning_rate: 1.0000e-05\n",
      "Epoch 28/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.1099 - mae: 4.6679 - val_loss: 48.7577 - val_mae: 4.5718 - learning_rate: 1.0000e-05\n",
      "Epoch 29/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8813 - mae: 4.6231 - val_loss: 48.7552 - val_mae: 4.5893 - learning_rate: 1.0000e-05\n",
      "Epoch 30/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9250 - mae: 4.6524 - val_loss: 48.7559 - val_mae: 4.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 31/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8149 - mae: 4.6286 - val_loss: 48.7556 - val_mae: 4.5826 - learning_rate: 1.0000e-05\n",
      "Epoch 32/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8301 - mae: 4.6298 - val_loss: 48.7554 - val_mae: 4.5850 - learning_rate: 1.0000e-05\n",
      "Epoch 33/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.7994 - mae: 4.6339 - val_loss: 48.7552 - val_mae: 4.5873 - learning_rate: 1.0000e-05\n",
      "Epoch 34/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9123 - mae: 4.6392 - val_loss: 48.7560 - val_mae: 4.5801 - learning_rate: 1.0000e-05\n",
      "Epoch 35/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8023 - mae: 4.6310 - val_loss: 48.7558 - val_mae: 4.5815 - learning_rate: 1.0000e-05\n",
      "Epoch 36/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.6347 - mae: 4.6287 - val_loss: 48.7552 - val_mae: 4.5876 - learning_rate: 1.0000e-05\n",
      "Epoch 37/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.7973 - mae: 4.6365 - val_loss: 48.7555 - val_mae: 4.5836 - learning_rate: 1.0000e-05\n",
      "Epoch 38/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8640 - mae: 4.6295 - val_loss: 48.7555 - val_mae: 4.5837 - learning_rate: 1.0000e-05\n",
      "Epoch 39/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8630 - mae: 4.6306 - val_loss: 48.7559 - val_mae: 4.6019 - learning_rate: 1.0000e-05\n",
      "Epoch 40/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.5757 - mae: 4.6439 - val_loss: 48.7552 - val_mae: 4.5924 - learning_rate: 1.0000e-05\n",
      "Epoch 41/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.8100 - mae: 4.6337 - val_loss: 48.7552 - val_mae: 4.5922 - learning_rate: 1.0000e-05\n",
      "Epoch 42/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.6527 - mae: 4.6528 - val_loss: 48.7557 - val_mae: 4.5818 - learning_rate: 1.0000e-05\n",
      "Epoch 43/100\n",
      "438/438 - 1s - 2ms/step - loss: 53.0492 - mae: 4.6556 - val_loss: 48.7556 - val_mae: 4.5828 - learning_rate: 1.0000e-05\n",
      "Epoch 44/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.7885 - mae: 4.6244 - val_loss: 48.7568 - val_mae: 4.5756 - learning_rate: 1.0000e-05\n",
      "Epoch 45/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.9227 - mae: 4.6332 - val_loss: 48.7552 - val_mae: 4.5878 - learning_rate: 1.0000e-05\n",
      "Epoch 46/100\n",
      "438/438 - 1s - 2ms/step - loss: 52.6634 - mae: 4.6323 - val_loss: 48.7555 - val_mae: 4.5986 - learning_rate: 1.0000e-05\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 42.8912 - mae: 4.4182\n",
      "Test Loss: 48.75515365600586\n",
      "Test MAE: 4.590336322784424\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, LSTM, Input, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Check the minimum sequence length\n",
    "min_seq_length = max_length\n",
    "print(f\"Minimum sequence length: {min_seq_length}\")\n",
    "\n",
    "# Define the CNN-LSTM model with appropriate kernel sizes\n",
    "model = Sequential([\n",
    "    Input(shape=(max_length, input_dim)),\n",
    "    Conv1D(32, 1, activation='relu'),  # Kernel size 1 to handle very short sequences\n",
    "    Dropout(0.3),\n",
    "    Conv1D(64, 1, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    Dense(64),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dense(32),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model with a smaller learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Callbacks for early stopping and reducing learning rate\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), \n",
    "                    callbacks=[early_stopping, reduce_lr], verbose=2)\n",
    "\n",
    "model.save(\"cnn_lstm_model.keras\")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test MAE: {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
